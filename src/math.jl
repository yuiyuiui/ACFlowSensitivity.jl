tolerance(T) = eps(real(T))^(1 // 2)
strict_tol(T) = eps(real(T))^(2 // 3)
relax_tol(T) = eps(real(T))^(1 // 4)

function integral(f::Function, a::T, b::T; h::T=T(1e-4)) where {T<:Real}
    n_raw = floor((b - a) / h)
    n = Int(n_raw)
    if isodd(n)
        n -= 1
    end
    if n < 2
        error("step is too large")
    end

    fa = f(a)
    !(typeof(fa) <: Union{T,Complex{T}}) &&
        error("Type of the output of f should be consistent with its input")
    fb = f(a + h * T(n))
    acc = fa + fb

    @inbounds for i in 1:(n - 1)
        x = a + h * T(i)
        coeff = isodd(i) ? T(4) : T(2)
        acc += coeff * f(x)
    end

    return acc * (h / T(3))
end

function Lp(f::Function, p::Real, a::T, b::T; h::T=T(1e-4)) where {T<:Real}
    Tp = T(p)
    return integral(x->abs(f(x))^Tp, a, b; h=h)^(1/Tp)
end

#-----------------------------------------

#= for poles in discrete situation
function kernel(Îµ::Float64)
    return continous_spectral_density([0.0], [Îµ], [1 / (sqrt(2Ï€) * Îµ)])
end
=#

#=
 Newton Method and curve fitting are copied from https://github.com/yuiyuiui/ACFlow
=#

# Newton Method
function _apply(feed::Vector{T}, f::Vector{T}, J::Matrix{T}) where {T}
    resid = nothing
    step = T(1)
    limit = T(1e-4)
    try
        resid = -pinv(J) * f
    catch
        resid = zeros(T, length(feed))
    end
    if any(x -> x > limit, abs.(feed))
        ratio = abs.(resid ./ feed)
        max_ratio = maximum(ratio[abs.(feed) .> limit])
        if max_ratio > T(1)
            step = T(1) / max_ratio
        end
    end
    return feed + step .* resid
end
function newton(fun::Function, grad::Function, guess::Vector{T}; maxiter::Int=20000,
                tol::T=T(1e-4)) where {T<:Real}
    counter = 0
    feed = copy(guess)
    f = fun(feed)
    J = grad(feed)
    back = _apply(feed, f, J)
    reach_tol = false

    while true
        counter = counter + 1
        feed += T(1//2) * (back - feed)

        f = fun(feed)
        J = grad(feed)
        back = _apply(feed, f, J)

        any(isnan.(back)) && error("Got NaN!")
        if counter > maxiter || maximum(abs.(back - feed)) < tol
            break
        end
    end

    if counter > maxiter
        println("Tolerance is reached in newton()!")
        @show norm(grad(back))
        reach_tol = true
    end

    return back, counter, reach_tol
end

# curve_fit is copied from https://github.com/yuiyuiui/ACFlow
"""
    OnceDifferentiable

Mutable struct. It is used for objectives and solvers where the gradient
is available/exists.

### Members
* â„±! -> Objective. It is actually a function call and return objective.
* ğ’¥! -> It is a function call as well and returns jacobian of objective.
* ğ¹  -> Cache for â„±! output.
* ğ½  -> Cache for ğ’¥! output.
"""
mutable struct OnceDifferentiable
    â„±!::Any
    ğ’¥!::Any
    ğ¹::Any
    ğ½::Any
end

"""
    OnceDifferentiable(ğ‘“, p0::AbstractArray, ğ¹::AbstractArray)

Constructor for OnceDifferentiable struct. `ğ‘“` is the function, `p0` is
the inital guess, `ğ¹ = ğ‘“(p0)` is the cache for `ğ‘“`'s output.
"""
function OnceDifferentiable(ğ‘“, p0::AbstractArray, ğ¹::AbstractArray)
    # Backup ğ‘“(x) to ğ¹.
    function â„±!(ğ¹, x)
        return copyto!(ğ¹, ğ‘“(x))
    end

    # Calculate jacobian for ğ‘“(x), the results are stored in ğ½.
    # The finite difference method is used.
    function ğ’¥!(ğ½, x)
        rel_step = cbrt(eps(real(eltype(x))))
        abs_step = rel_step
        @inbounds for i in 1:length(x)
            xâ‚› = x[i]
            Ïµ = max(rel_step * abs(xâ‚›), abs_step)
            x[i] = xâ‚› + Ïµ
            fâ‚‚ = vec(ğ‘“(x))
            x[i] = xâ‚› - Ïµ
            fâ‚ = vec(ğ‘“(x))
            ğ½[:, i] = (fâ‚‚ - fâ‚) ./ (2 * Ïµ)
            x[i] = xâ‚›
        end
    end

    # Create memory space for jacobian matrix
    ğ½ = eltype(p0)(NaN) .* vec(ğ¹) .* vec(p0)'

    # Call the default constructor
    return OnceDifferentiable(â„±!, ğ’¥!, ğ¹, ğ½)
end

"""
    value(obj::OnceDifferentiable)

Return `obj.ğ¹`. `obj` will not be affected.
"""
value(obj::OnceDifferentiable) = obj.ğ¹

"""
    value(obj::OnceDifferentiable, ğ¹, x)

Return `ğ‘“(x)`. `obj` will not be affected, but `ğ¹` is updated.
"""
value(obj::OnceDifferentiable, ğ¹, x) = obj.â„±!(ğ¹, x)

"""
    value!(obj::OnceDifferentiable, x)

Return `ğ‘“(x)`. `obj.ğ¹` will be updated and returned.
"""
function value!(obj::OnceDifferentiable, x)
    obj.â„±!(obj.ğ¹, x)
    return obj.ğ¹
end

"""
    jacobian(obj::OnceDifferentiable)

Return `obj.ğ½`. `obj` will not be affected.
"""
jacobian(obj::OnceDifferentiable) = obj.ğ½

"""
    jacobian(obj::OnceDifferentiable, ğ½, x)

Return jacobian. `obj` will not be affected, but `ğ½` is updated.
"""

jacobian(obj::OnceDifferentiable, ğ½, x) = obj.ğ’¥!(ğ½, x)

"""
    jacobian!(obj::OnceDifferentiable, x)

Return jacobian. `obj.ğ½` will be updated and returned.
"""
function jacobian!(obj::OnceDifferentiable, x)
    obj.ğ’¥!(obj.ğ½, x)
    return obj.ğ½
end

"""
    LMOptimizationResults{T,N}

It is used to save the optimization results of the `levenberg_marquardt`
algorithm.

### Members
* xâ‚€         -> Initial guess for the solution.
* minimizer  -> Final results for the solution.
* minimum    -> Residual.
* iterations -> Number of iterations.
* xconv      -> If the convergence criterion 1 is satisfied.
* gconv      -> If the convergence criterion 2 is satisfied.
"""
struct LMOptimizationResults{T,N}
    xâ‚€::Array{T,N}
    minimizer::Array{T,N}
    minimum::T
    iterations::Int
    xconv::Bool
    gconv::Bool
end

"""
    levenberg_marquardt(df::OnceDifferentiable, xâ‚€::AbstractVector{T})

Returns the argmin over x of `sum(f(x).^2)` using the Levenberg-Marquardt
algorithm. The function `f` is encoded in `df`. `xâ‚€` is an initial guess
for the solution.

See also: [`OnceDifferentiable`](@ref).
"""
function levenberg_marquardt(df::OnceDifferentiable, xâ‚€::AbstractVector{T} where {T})
    # Some predefined constants
    min_diagonal = 1e-6 # lower bound on values of diagonal matrix
    #
    x_tol = 1e-08 # search tolerance in x
    g_tol = 1e-12 # search tolerance in gradient
    maxIter = 1000  # maximum number of iterations
    #
    Î›â‚˜ = 1e+16 # minimum trust region radius
    Î»â‚˜ = 1e-16 # maximum trust region radius
    Î» = eltype(xâ‚€)(10) # (inverse of) initial trust region radius
    Î»áµ¢ = 10.0  # Î» is multiplied by this factor after step below min quality
    Î»áµ£ = 0.10  # Î» is multiplied by this factor after good quality steps
    #
    min_step_quality = 1e-3 # for steps below this quality, the trust region is shrinked
    good_step_quality = 0.75 # for steps above this quality, the trust region is expanded

    # First evaluation
    # Both df.ğ¹ and df.ğ½ are updated.
    # And ğ¹ and ğ½ become aliases of df.ğ¹ and df.ğ½, respectively.
    value!(df, xâ‚€)
    jacobian!(df, xâ‚€)
    ğ¹ = value(df)
    ğ½ = jacobian(df)

    # Setup convergence criteria
    converged = false
    xconv = false
    gconv = false
    iter = 0

    # Calculate ğ‘“(xâ‚€) and initial residual
    x = copy(xâ‚€)
    trial_f = similar(ğ¹)
    C_resid = sum(abs2, ğ¹)

    # Create buffers
    ğ½áµ€ğ½ = diagm(x)
    ğ½Î´x = similar(ğ¹)

    # Main iteration
    while (~converged && iter < maxIter)
        # Update jacobian ğ½ for new x
        jacobian!(df, x)

        # Solve the equation: [ğ½áµ€ğ½ + Î» diag(ğ½áµ€ğ½)] Î´ = ğ½áµ€ğ¹
        # What we want to get is Î´.
        mul!(ğ½áµ€ğ½, ğ½', ğ½)
        #
        ğ·áµ€ğ· = diag(ğ½áµ€ğ½)
        replace!(x -> x â‰¤ min_diagonal ? min_diagonal : x, ğ·áµ€ğ·)
        #
        @simd for i in eachindex(ğ·áµ€ğ·)
            @inbounds ğ½áµ€ğ½[i, i] += Î» * ğ·áµ€ğ·[i]
        end
        #
        Î´x = - ğ½áµ€ğ½ \ (ğ½' * ğ¹)

        # If the linear assumption is valid, the new residual is predicted.
        mul!(ğ½Î´x, ğ½, Î´x)
        ğ½Î´x .= ğ½Î´x .+ ğ¹
        P_resid = sum(abs2, ğ½Î´x)

        # Try to calculate new x, and then ğ¹ â‰¡ ğ‘“(x), and then the residual.
        xnew = x + Î´x
        value(df, trial_f, xnew)
        T_resid = sum(abs2, trial_f)

        # Step quality = residual change / predicted residual change
        Ï = (T_resid - C_resid) / (P_resid - C_resid)
        if Ï > min_step_quality
            # Update x, ğ‘“(x), and residual.
            x .= xnew
            value!(df, x)
            C_resid = T_resid

            # Increase trust region radius
            if Ï > good_step_quality
                Î» = max(Î»áµ£ * Î», Î»â‚˜)
            end
        else
            # Decrease trust region radius
            Î» = min(Î»áµ¢ * Î», Î›â‚˜)
        end

        # Increase the iteration
        iter += 1

        # Check convergence criteria:
        # 1. Small gradient: norm(ğ½áµ€ * ğ¹, Inf) < g_tol
        if norm(ğ½' * ğ¹, Inf) < g_tol
            gconv = true
        end
        # 2. Small step size: norm(Î´x) < x_tol
        if norm(Î´x) < x_tol * (x_tol + norm(x))
            xconv = true
        end
        # 3. Calculate converged
        converged = gconv | xconv
    end

    # Return the results
    return LMOptimizationResults(xâ‚€,      # xâ‚€
                                 x,       # minimizer
                                 C_resid, # residual
                                 iter,    # iterations
                                 xconv,   # xconv
                                 gconv)
end

"""
    LsqFitResult

It encapsulates the results for curve fitting.

### Members
* param     -> Fitted results, i.e, the fitting parameters.
* resid     -> Residuals.
* jacobian  -> Jacobian matrix.
* converged -> If the curve-fitting algorithm is converged.
"""
struct LsqFitResult
    param::Any
    resid::Any
    jacobian::Any
    converged::Any
end

"""
    curve_fit(model, x, y, p0)

Fit data to a non-linear `model`. `p0` is an initial model parameter guess.
The return object is a composite type (`LsqFitResult`).

See also: [`LsqFitResult`](@ref).
"""
function curve_fit(model, x::AbstractArray, y::AbstractArray, p0::AbstractArray)
    f = (p) -> model(x, p) - y
    r = f(p0)
    R = OnceDifferentiable(f, p0, r)
    OR = levenberg_marquardt(R, p0)
    p = OR.minimizer
    conv = OR.xconv || OR.gconv
    return LsqFitResult(p, value!(R, p), jacobian!(R, p), conv)
end

# partial differentiation of sigmoid loss function
function âˆ‚Â²lossÏ•Divâˆ‚pÂ²(p::Vector{T}, x::Vector{T}, y::Vector{T}) where {T<:Real}
    a, b, c, d = p
    L = length(x)
    s = 1 ./ (1 .+ exp.(-d * (x .- c)))
    s1 = s .* (1 .- s)  # s1 = s * (1 - s)
    r = a .+ b * s .- y  # æ®‹å·®é¡¹

    Jaa = 2 * L
    Jbb = 2 * sum(s .^ 2)
    Jcc = 2 * b^2 * d^2 * sum(s .^ 2 .* (1 .- s) .^ 2) +
          2 * b * d^2 * sum(s1 .* (1 .- 2 * s) .* r)
    Jdd = 2 * sum(b^2 * s .^ 2 .* (1 .- s) .^ 2 .* (x .- c) .^ 2 +
                  b * (x .- c) .^ 2 .* s1 .* (1 .- 2 * s) .* r)

    Jab = 2 * sum(s)
    Jac = -2 * b * d * sum(s1)
    Jad = 2 * b * sum(s1 .* (x .- c))
    Jbc = -2 * d * sum(s1 .* (b * s .+ r))
    Jbd = 2 * sum(s1 .* (x .- c) .* (b * s .+ r))
    Jcd = -2 *
          b *
          sum(s1 .* (b * d * s1 .* (x .- c) .+ (1 .+ d * (x .- c) .* (1 .- 2 * s)) .* r))

    return [Jaa Jab Jac Jad; Jab Jbb Jbc Jbd; Jac Jbc Jcc Jcd; Jad Jbd Jcd Jdd]
end

function âˆ‚Â²lossÏ•Divâˆ‚pâˆ‚y(p::Vector{T}, x::Vector{T}, y::Vector{T}) where {T<:Real}
    a, b, c, d = p
    L = length(x)

    s = 1 ./ (1 .+ exp.(-d * (x .- c)))
    s1 = s .* (1 .- s)  # s1 = s * (1 - s)

    âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix = zeros(T, 4, L)

    âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix[1, :] .= -2  # âˆ‚Â²loss/âˆ‚aâˆ‚y_i
    âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix[2, :] = -2 * s  # âˆ‚Â²loss/âˆ‚bâˆ‚y_i
    âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix[3, :] = 2 * b * d * s1  # âˆ‚Â²loss/âˆ‚câˆ‚y_i
    âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix[4, :] = -2 * b * s1 .* (x .- c)  # âˆ‚Â²loss/âˆ‚dâˆ‚y_i

    return âˆ‚Â²loss_âˆ‚pâˆ‚y_matrix
end

# calculate jacobian with finite-difference. Borrowed form https://github.com/yuiyuiui/ACFlow
# it accepts function that maps vector to vector or number
Base.vec(x::Number) = [x]
function fdgradient(f::Function, x::Vector{T}) where {T<:Number}
    J = zeros(T, length(f(x)), length(x))
    rel_step = cbrt(eps(real(eltype(x))))
    abs_step = rel_step
    @inbounds for i in 1:length(x)
        xâ‚› = x[i]
        Ïµ = max(rel_step * abs(xâ‚›), abs_step)
        x[i] = xâ‚› + Ïµ
        yâ‚‚ = vec(f(x))
        x[i] = xâ‚› - Ïµ
        yâ‚ = vec(f(x))
        J[:, i] .+= (yâ‚‚ - yâ‚) ./ (2 * Ïµ)
        x[i] = xâ‚›
    end
    T<:Complex && @inbounds for i in 1:length(x)
        xâ‚› = x[i]
        Ïµ = max(rel_step * abs(xâ‚›), abs_step)
        x[i] = xâ‚› + im * Ïµ
        yâ‚‚ = vec(f(x))
        x[i] = xâ‚› - im * Ïµ
        yâ‚ = vec(f(x))
        J[:, i] .+= im * (yâ‚‚ - yâ‚) ./ (2 * Ïµ)
        x[i] = xâ‚›
    end
    return J
end

# gradient of L2 loss function(vector to vector)
# C^n -> R^m -> loss, and this interface is easy to generalize
function âˆ‡L2loss(J::Matrix{T}, w::Vector{R}) where {T<:Number,R<:Real}
    @assert R == real(T)
    n  = size(J,2)
    Dsw = Diagonal(sqrt.(w))
    _, S, V = svd(Dsw * hcat(real(J),imag(J)))
    T<:Real && return S[1], V[1:n,1] * S[1]
    return S[1], (V[1:n,1] + im * V[n+1:2n,1]) * S[1]
end
